{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c760d98b",
   "metadata": {},
   "source": [
    " ### step-by-step workflow for:\n",
    "\n",
    "‚úÖ Reading a PDF ‚Üí\n",
    "\n",
    "‚úÖ Extracting text, images, and tables ‚Üí\n",
    "\n",
    "‚úÖ Chunking and embedding each separately ‚Üí\n",
    "\n",
    "‚úÖ Uploading to Qdrant with separate collections "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1dff62",
   "metadata": {},
   "source": [
    " #### Step-by-Step Pipeline: PDF ‚Üí Qdrant with Embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5c45c8",
   "metadata": {},
   "source": [
    " #### ‚úÖ 1. Install Required Libraries\n",
    "* pip install langchain qdrant-client pdfplumber pytesseract pillow sentence-transformers unstructured opencv-python "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8629a92c",
   "metadata": {},
   "source": [
    " #### Create Qdrant client using qdrant API key and endpoint of your cluster  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "335aa1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ram\\AppData\\Local\\Temp\\ipykernel_13088\\2343988990.py:15: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  client.recreate_collection(\n",
      "C:\\Users\\Ram\\AppData\\Local\\Temp\\ipykernel_13088\\2343988990.py:21: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  client.recreate_collection(\n",
      "C:\\Users\\Ram\\AppData\\Local\\Temp\\ipykernel_13088\\2343988990.py:27: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  client.recreate_collection(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import uuid\n",
    "from httpx import Timeout\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # Embedding model\n",
    "\n",
    "## create Qdrant client usking qdrant API key and endpoint of your cluster\n",
    "client = QdrantClient(\n",
    "    url=\"https://c5347484-117d-4448-a8a0-ad01951fbb6d.us-west-2-0.aws.cloud.qdrant.io\",  # Replace with your endpoint\n",
    "    api_key=\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.P3r568LxWUP0PHSS6FLp09G7jTNKdd40PgrmEL4hUJI\"  # Replace with your Qdrant Cloud API key\n",
    ")\n",
    "############################### Create a collection (if it doesn't exist)\n",
    "client.recreate_collection(\n",
    "    collection_name=\"tables_collection\",\n",
    "    vectors_config=VectorParams(size=384, distance=Distance.COSINE),\n",
    ")\n",
    "\n",
    "############################### Create a collection (if it doesn't exist)\n",
    "client.recreate_collection(\n",
    "    collection_name=\"Image_collection\",\n",
    "    vectors_config=VectorParams(size=384, distance=Distance.COSINE),\n",
    ")\n",
    "\n",
    "############################### Create a collection (if it doesn't exist)\n",
    "client.recreate_collection(\n",
    "    collection_name=\"Text_collection\",\n",
    "    vectors_config=VectorParams(size=384, distance=Distance.COSINE),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e727344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# client.delete_collection(collection_name=\"tables_collection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e61d85d",
   "metadata": {},
   "source": [
    " ###  Image Captioning with Gemini Pro Vision\n",
    "\n",
    "* To do image captioning using Gemini 1.5 or 2.0 (Gemini Pro Vision), you‚Äôll use Google's google-generativeai Python SDK with your API key. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856ed18a",
   "metadata": {},
   "source": [
    " üîë Get Your Gemini API Key\n",
    "1. Go to: https://makersuite.google.com/app/apikey\n",
    "2. Sign in with your Google account\n",
    "3. Copy your API key "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ee9f26bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from PIL import Image\n",
    "\n",
    "############################### Configure with your API key  ##############################\n",
    "genai.configure(api_key=\"AIzaSyBQ4pg7W6kb0gE8R69ZI8L2g9KZgVPeuNI\")\n",
    "\n",
    "############################### Load Gemini multimodal model  ##############################\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "\n",
    "############################## Load Embedding Models   ##############################\n",
    "text_model = SentenceTransformer('all-MiniLM-L6-v2')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e33243bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### Import necessary libraries  ##############################\n",
    "import fitz  # PyMuPDF\n",
    "from PIL import Image\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "\n",
    "doc = fitz.open(\"pdf/GSDP project APS.pdf\")\n",
    "output_folder = \"extract_imgs\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "###############################*****************8888888************************#############################\n",
    "# Define minimum width and height for images to be extracted\n",
    "# You can adjust these values based on your requirements\n",
    "\n",
    "MIN_WIDTH = 300\n",
    "MIN_HEIGHT = 200\n",
    "\n",
    "############################## Extract images from PDF  ##############################\n",
    "# Initialize lists to store image paths and page numbers\n",
    "saved_images = []  \n",
    "page_No=[]\n",
    "for page_index, page in enumerate(doc):\n",
    "    page_number = page.number + 1  # Page numbers are 0-indexed in PyMuPDF\n",
    "    image_metadata_list = page.get_images(full=True)\n",
    "\n",
    "    for img_index, img in enumerate(image_metadata_list):\n",
    "        xref = img[0]\n",
    "        base_image = doc.extract_image(xref)\n",
    "        image_bytes = base_image[\"image\"]\n",
    "        image_ext = base_image[\"ext\"]\n",
    "        width = base_image[\"width\"]\n",
    "        height = base_image[\"height\"]\n",
    "\n",
    "        if width >= MIN_WIDTH and height >= MIN_HEIGHT:\n",
    "            \n",
    "            filename = f\"page{page_index+1}_img{img_index+1}_{width}x{height}.{image_ext}\"\n",
    "            filepath = os.path.join(output_folder, filename)\n",
    "\n",
    "            with open(filepath, \"wb\") as f:\n",
    "                f.write(image_bytes)\n",
    "            # Append the saved image path and page number to the lists\n",
    "            image = Image.open(filepath)\n",
    "            saved_images.append(image)\n",
    "            page_No.append(page_number)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440cc31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import io\n",
    "import base64\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=256,\n",
    "    chunk_overlap=20\n",
    ")  \n",
    "for i in range(13,len(saved_images)):\n",
    "    # Load the image\n",
    "    image = saved_images[i]\n",
    "    image = image.convert(\"RGB\")  # Convert the first image to RGB\n",
    "    \n",
    "    img_byte_arr = io.BytesIO()\n",
    "    image.save(img_byte_arr, format='PNG')\n",
    "    image_bytes = img_byte_arr.getvalue()\n",
    "    image_base64 = base64.b64encode(image_bytes).decode(\"utf-8\")\n",
    "    # Generate caption\n",
    "    # Note: The model.generate_content method may vary based on the library version\n",
    "    response = model.generate_content([\"give a interpretations on graph:\", image],\n",
    "                                      generation_config={\"max_output_tokens\": 150,  # üîπ Set the token limit here\n",
    "                                                                \"temperature\": 0.7  # Optional: control creativity\n",
    "                                                                }\n",
    "                                                                )\n",
    "    chunks = splitter.split_text(response.text)\n",
    "    points = []\n",
    "    for chunk in chunks:\n",
    "        vec = text_model.encode(chunk).tolist()  # üëà Single vector #embedding model (text_model)\n",
    "        point = PointStruct(\n",
    "            id=i+1,\n",
    "            vector=vec,\n",
    "            payload={\n",
    "                \"chunk\": chunk,\n",
    "                \"Image\": image_base64, # ‚úÖ JSON-safe string\n",
    "                \"type\": \"Image\",\n",
    "                \"page_number\": page_No[i]\n",
    "            }\n",
    "        )\n",
    "        points.append(point)\n",
    "    # Upload all points at once\n",
    "    client.upsert(\n",
    "        collection_name=\"Image_collection\",\n",
    "        points=points\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d189d5d",
   "metadata": {},
   "source": [
    " ### Extract text from PDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0c2c6de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "# Step 1: Choose a model (e.g., MiniLM used in RAG)\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "doc = fitz.open(\"pdf/GSDP project APS.pdf\")\n",
    "extract_text = []  \n",
    "page_No=[]\n",
    "for page_num, page in enumerate(doc):\n",
    "    text = page.get_text()\n",
    "    page_No.append(page_num + 1)\n",
    "    extract_text.append(text)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "86d4e3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Encode the text using the tokenizer\n",
    "BATCH_SIZE = 100\n",
    "points = []\n",
    "\n",
    "for i in range(len(extract_text)):\n",
    "    chunks = splitter.split_text(extract_text[i])\n",
    "    for j, chunk in enumerate(chunks):\n",
    "        vec = text_model.encode(chunk).tolist()\n",
    "        point = PointStruct(\n",
    "            id=i+1,  # ‚úÖ FIXED: valid UUID\n",
    "            vector=vec,\n",
    "            payload={\n",
    "                \"chunk\": chunk,\n",
    "                \"type\": \"Text\",\n",
    "                \"page_number\": page_No[i]\n",
    "            }\n",
    "        )\n",
    "        points.append(point)\n",
    "\n",
    "############################### ‚úÖ Upload in Batches   ##############################\n",
    "for i in range(0, len(points), BATCH_SIZE):\n",
    "    batch = points[i:i + BATCH_SIZE]\n",
    "    client.upsert(\n",
    "        collection_name=\"Text_collection\",\n",
    "        points=batch\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfca97a",
   "metadata": {},
   "source": [
    "### extract table from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "13171061",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot set gray non-stroke color because /'P98' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P260' is an invalid float value\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "# Extract tables from PDF using pdfplumber\n",
    "# Initialize an empty list to store tables\n",
    "tables = []\n",
    "page_No=[]\n",
    "with pdfplumber.open(\"pdf/GSDP project APS.pdf\")  as pdf:\n",
    "    for page in pdf.pages:\n",
    "        # print(page.page_number)\n",
    "        page_tables = page.extract_tables()\n",
    "        for table in page_tables:\n",
    "            if table:  # skip empty\n",
    "                tables.append(table)\n",
    "                page_No.append(page.page_number)  # Store the page number\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1618dae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 17)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tables) , len(page_No)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "983f8ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Upload successful!\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client.models import PointStruct\n",
    "\n",
    "# Prepare points for Qdrant\n",
    "# Each point will contain the table data as a CSV string and its vector representation\n",
    "points = []\n",
    "\n",
    "for i in range(len(tables)):\n",
    "    table=tables[i]\n",
    "    df = pd.DataFrame(table[1:], columns=table[0])\n",
    "    table_chunk = df.to_csv(index=False)\n",
    "    vec=text_model.encode(table_chunk, convert_to_tensor=True).tolist() \n",
    "    point = PointStruct(\n",
    "        id=i + 1,  # unique ID\n",
    "        vector=vec,\n",
    "        payload={\"chunk\": table_chunk, \"type\": \"table\", \"page number\": page_No[i]}\n",
    "    )\n",
    "\n",
    "    points.append(point)\n",
    "\n",
    "############################### Upload all points at once   ##############################\n",
    "try:\n",
    "    client.upsert(collection_name=\"tables_collection\", points=points)\n",
    "    print(\"‚úÖ Upload successful!\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Upload failed:\", e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da584479",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
